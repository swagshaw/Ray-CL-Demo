{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ray Tune from scratch and existing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN76L3hn27Un2rEQ28s/MzV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swagshaw/Ray-CL-Demo/blob/main/Ray_Tune_continual_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Ym_OIbm9Sd"
      },
      "source": [
        "这个例子是关于如何使用Ray Tune在Ray Serve上实现CL机制。\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa5gLqs0likh",
        "outputId": "740103f6-e330-4105-bc96-2970be31f11e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 12 12:15:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBYZd12zmIpl",
        "outputId": "61d8e361-5200-4b1c-c068-99cc2b045a37"
      },
      "source": [
        "!pip install torch\n",
        "!pip install ray\n",
        "!pip install ray[tune]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting ray\n",
            "  Downloading ray-1.9.2-cp37-cp37m-manylinux2014_x86_64.whl (57.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 57.6 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.4.2)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.1.0-py3-none-any.whl (171 kB)\n",
            "\u001b[K     |████████████████████████████████| 171 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.43.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray) (4.10.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray) (21.3)\n",
            "Collecting deprecated>=1.2.3\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray) (1.13.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->redis>=3.5.0->ray) (3.0.6)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.0)\n",
            "Installing collected packages: deprecated, redis, ray\n",
            "Successfully installed deprecated-1.2.13 ray-1.9.2 redis-4.1.0\n",
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.7/dist-packages (1.9.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.43.0)\n",
            "Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.13)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (21.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.4.2)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.9)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[tune]) (1.15.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (21.3)\n",
            "Requirement already satisfied: deprecated>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (1.2.13)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (4.10.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[tune]) (1.13.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->redis>=3.5.0->ray[tune]) (3.0.6)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.18.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "安装完必备的依赖，小A的故事正式开始。"
      ],
      "metadata": {
        "id": "jp5JRW5eVsgL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScI7DZlFmzNk"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "from functools import partial\n",
        "from math import ceil\n",
        "import torch\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import ray\n",
        "from ray import tune\n",
        "import shutil\n",
        "import json\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "from torch.utils.data import random_split, Subset\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import transforms\n",
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbZ5QFYknS4X"
      },
      "source": [
        "小A是一所公司新入职的算法工程师，刚入职不久就接到了产品经理的一个需求，维护一个部署在Ray Serve上的推理模型，监控它的性能变化。\n",
        "这里我们使用基于mnist数据集训练的CNN模型为例： "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLA-Vgr5nLqr"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, layer_size=192):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer_size = layer_size\n",
        "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
        "        self.fc = nn.Linear(192, self.layer_size)\n",
        "        self.out = nn.Linear(self.layer_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
        "        x = x.view(-1, 192)\n",
        "        x = self.fc(x)\n",
        "        x = self.out(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_loader, device=None):\n",
        "    device = device or torch.device(\"cpu\")\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def test(model, data_loader, device=None):\n",
        "    device = device or torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQpipazGnaeA"
      },
      "source": [
        "Data interface:To get data from MINST dataset\n",
        "\n",
        "这是一个用于模拟每天到来数据的MNIST interface， 函数简介如下：\n",
        "\n",
        "\n",
        "*   def _get_day_slice(self, day=0):\n",
        "\n",
        "\n",
        "> 用于将数据集切分，初始拥有30%数据，剩余数据均分给每一天，模拟每日到来的新数据。\n",
        "\n",
        "\n",
        "*   def get_data(self, day=0):\n",
        "\n",
        "> 用于获得迄今为止的全部数据。\n",
        "\n",
        "\n",
        "* def get_incremental_data(self, day=0):\n",
        "\n",
        "> 用于获得某一天的新数据。并将它与初始数据，即记忆数据结合起来。例如day=3，此时就是第2天到第3天之间新获得的数据和第0天的初始数据。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut5iDy0YnnhM"
      },
      "source": [
        "class MNISTDataInterface(object):\n",
        "    \"\"\"Data interface. Simulates that new data arrives every day.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, max_days=10):\n",
        "        self.data_dir = data_dir\n",
        "        self.max_days = max_days\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        self.dataset = MNIST(\n",
        "            self.data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    def _get_day_slice(self, day=0):\n",
        "        if day < 0:\n",
        "            return 0\n",
        "        n = len(self.dataset)\n",
        "        # Start with 30% of the data, get more data each day\n",
        "        return min(n, ceil(n * (0.3 + 0.7 * day / self.max_days)))\n",
        "\n",
        "    def get_data(self, day=0):\n",
        "        \"\"\"Get complete normalized train and validation data to date.\"\"\"\n",
        "        end = self._get_day_slice(day)\n",
        "\n",
        "        available_data = Subset(self.dataset, list(range(end)))\n",
        "        train_n = int(0.8 * end)  # 80% train data, 20% validation data\n",
        "\n",
        "        return random_split(available_data, [train_n, end - train_n])\n",
        "\n",
        "    def get_incremental_data(self, day=0):\n",
        "        \"\"\"Get next normalized train and validation data day slice and merge with rehearsal data.\"\"\"\n",
        "        start = self._get_day_slice(day - 1)\n",
        "        end = self._get_day_slice(day)\n",
        "\n",
        "        rehearsal_data = Subset(self.dataset, list(range(self._get_day_slice(0))))\n",
        "        available_data = Subset(self.dataset, list(range(start, end)))\n",
        "        train_n = int(\n",
        "            0.8 * ((end - start)+len(rehearsal_data)))  # 80% train data, 20% validation data\n",
        "\n",
        "        return random_split(available_data+rehearsal_data, [train_n, end - start+len(rehearsal_data) - train_n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8uN4nKbodQU"
      },
      "source": [
        "Trainable API for Ray Tune\n",
        "\n",
        "这是一个用于训练的函数，基于Ray Tune，它会按照Ray Tune分发的config进行指定epoch次的训练，并将训练结果报告给Ray Tune，基于此来做出超参数搜索等操作。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0grJP0JQn3xk"
      },
      "source": [
        "def train_mnist(config,\n",
        "                start_model=None,\n",
        "                checkpoint_dir=None,\n",
        "                num_epochs=10,\n",
        "                use_gpus=False,\n",
        "                data_fn=None,\n",
        "                day=0):\n",
        "    # Create model\n",
        "    use_cuda = use_gpus and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    model = ConvNet(layer_size=config[\"layer_size\"]).to(device)\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
        "\n",
        "    # Load checkpoint, or load start model if no checkpoint has been\n",
        "    # passed and a start model is specified\n",
        "    load_dir = None\n",
        "    if checkpoint_dir:\n",
        "        load_dir = checkpoint_dir\n",
        "    elif start_model:\n",
        "        load_dir = start_model\n",
        "\n",
        "    if load_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(load_dir, \"checkpoint\"))\n",
        "        model.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    # Get full training datasets\n",
        "    train_dataset, validation_dataset = data_fn(day=day)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        validation_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    for i in range(num_epochs):\n",
        "        train(model, optimizer, train_loader, device)\n",
        "        acc = test(model, validation_loader, device)\n",
        "        if i == num_epochs - 1:\n",
        "            with tune.checkpoint_dir(step=i) as checkpoint_dir:\n",
        "                torch.save((model.state_dict(), optimizer.state_dict()),\n",
        "                           os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "            tune.report(mean_accuracy=acc, done=True)\n",
        "        else:\n",
        "            tune.report(mean_accuracy=acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cFGbDspopvj"
      },
      "source": [
        "Tune from scratch\n",
        "\n",
        "从头开始训练，使用当前获得的全部数据，对模型进行重新训练。通过Ray Tune依据config进行超参数搜索，记录下性能最好的一组超参数和模型保存下来。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V1kNsEZoi4P"
      },
      "source": [
        "def tune_from_scratch(num_samples=10, num_epochs=10, gpus_per_trial=0., day=0):\n",
        "    data_interface = MNISTDataInterface(\"~/data\", max_days=10)\n",
        "    num_examples = data_interface._get_day_slice(day)\n",
        "    #进行超参数搜索的参数空间\n",
        "    config = {\n",
        "        \"batch_size\": tune.choice([16, 32, 64]),\n",
        "        \"layer_size\": tune.choice([32, 64, 128, 192]),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"momentum\": tune.uniform(0.1, 0.9),\n",
        "    }\n",
        "    #基于metric做schedule\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"mean_accuracy\",\n",
        "        mode=\"max\",\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        parameter_columns=[\"layer_size\", \"lr\", \"momentum\", \"batch_size\"],\n",
        "        metric_columns=[\"mean_accuracy\", \"training_iteration\"])\n",
        "\n",
        "    analysis = tune.run(\n",
        "        partial(\n",
        "            train_mnist,# 封装好的训练函数\n",
        "            start_model=None,\n",
        "            data_fn=data_interface.get_data,\n",
        "            num_epochs=num_epochs,\n",
        "            use_gpus=True if gpus_per_trial > 0 else False,\n",
        "            day=day),\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 1,\n",
        "            \"gpu\": gpus_per_trial\n",
        "        },\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        verbose=0,\n",
        "        name=\"tune_serve_mnist_fromscratch\")\n",
        "\n",
        "    best_trial = analysis.get_best_trial(\"mean_accuracy\", \"max\", \"last\")\n",
        "    best_accuracy = best_trial.metric_analysis[\"mean_accuracy\"][\"last\"]\n",
        "    best_trial_config = best_trial.config\n",
        "    best_checkpoint = best_trial.checkpoint.value\n",
        "\n",
        "    return best_accuracy, best_trial_config, best_checkpoint, num_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "（Optional）将checkpoint移动到头节点上的“model_dir”。 当您想在多个节点上运行 Serve，您可能需要checkpoint移动到共享存储，例如 Amazon S3。\n"
      ],
      "metadata": {
        "id": "qaHFnz3pXz_V"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMB0uEVl2Eub"
      },
      "source": [
        "def _move_checkpoint_to_model_dir(model_dir, checkpoint, config, metrics):\n",
        "    \"\"\"Move backend checkpoint to a central `model_dir` on the head node.\n",
        "    If you would like to run Serve on multiple nodes, you might want to\n",
        "    move the checkpoint to a shared storage, like Amazon S3, instead.\"\"\"\n",
        "    os.makedirs(model_dir, 0o755, exist_ok=True)\n",
        "    print(\"Serving checkpoint: {}\".format(checkpoint))\n",
        "    checkpoint_path = os.path.join(model_dir, \"checkpoint\")\n",
        "    meta_path = os.path.join(model_dir, \"meta.json\")\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        shutil.rmtree(checkpoint_path)\n",
        "\n",
        "    shutil.copytree(checkpoint, checkpoint_path)\n",
        "\n",
        "    with open(meta_path, \"wt\") as fp:\n",
        "        json.dump(dict(config=config, metrics=metrics), fp)\n",
        "\n",
        "    return checkpoint_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里通过tune_from_scratch（）使用全量数据训练出模型并将其最佳参数保存起来。"
      ],
      "metadata": {
        "id": "xeA4I-BKJJfb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCDot9YQouuZ",
        "outputId": "caf9a9db-a567-47e2-8e7f-a11d04fef078"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    day = 7 # start training job from day 7\n",
        "    ray.shutdown()\n",
        "    ray.init(namespace=\"tune-serve-integration\") #初始化ray集群\n",
        "\n",
        "    model_dir = os.path.expanduser(\"~/mnist_tune_serve\")\n",
        "\n",
        "    gpus_per_trial = 0.25\n",
        "    serve_gpu = True if gpus_per_trial > 0 else False\n",
        "    num_samples = 1\n",
        "    num_epochs = 1 \n",
        "\n",
        "    # for early stopping\n",
        "    # train everyday from scratch\n",
        "    print(\"Start training job from scratch on day {}.\".format(day))\n",
        "    acc, config, best_checkpoint, num_examples = tune_from_scratch(\n",
        "        num_samples, num_epochs, gpus_per_trial, day=day)\n",
        "    print(\"Trained day {} from scratch on {} samples. \"\n",
        "          \"Best accuracy: {:.4f}. Best config: {}\".format(\n",
        "        day, num_examples, acc, config))\n",
        "    _move_checkpoint_to_model_dir(model_dir,\n",
        "                                  best_checkpoint,\n",
        "                                  config,\n",
        "                                  acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-12 13:03:14,447\tWARNING experiment.py:256 -- No name detected on trainable. Using DEFAULT.\n",
            "2022-01-12 13:03:14,449\tINFO registry.py:70 -- Detected unknown callable for trainable. Converting to class.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training job from scratch on day 7.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-12 13:03:17,105\tWARNING worker.py:1245 -- Warning: The actor ImplicitFunc is very large (45 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
            "2022-01-12 13:03:17,293\tWARNING util.py:166 -- The `start_trial` operation took 1.214 s, which may be a performance bottleneck.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained day 7 from scratch on 47400 samples. Best accuracy: 0.9354. Best config: {'batch_size': 32, 'layer_size': 64, 'lr': 0.027941258257921566, 'momentum': 0.46977404378785503}\n",
            "Serving checkpoint: /root/ray_results/tune_serve_mnist_fromscratch/DEFAULT_0119b_00000_0_batch_size=32,layer_size=64,lr=0.027941,momentum=0.46977_2022-01-12_13-03-16/checkpoint_000000/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在我们已经拥有了需要进行微调的模型，以及它配套的训练，测试函数。只需要每七天进行一次重新训练就可以完成需求了。"
      ],
      "metadata": {
        "id": "apHiEUTgJli6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2-ARhU0uOBR"
      },
      "source": [
        "And after you tune from scratch, you can use above code to implement incremental training. Fisrt, we need a tune.run() used old config and model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在你从头开始全量训练之后，你可以使用以下代码来实现基于rehearsal的持续学习。tune的部分没什么两样。"
      ],
      "metadata": {
        "id": "0gX6ApmrZA64"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIIREmrzvHM-"
      },
      "source": [
        "def tune_from_existing(start_model,\n",
        "                       start_config,\n",
        "                       num_samples=10,\n",
        "                       num_epochs=10,\n",
        "                       gpus_per_trial=0.,\n",
        "                       day=0):\n",
        "    data_interface = MNISTDataInterface(\"/tmp/mnist_data\", max_days=10)\n",
        "    num_examples = data_interface._get_day_slice(day) - \\\n",
        "                   data_interface._get_day_slice(day - 1)+data_interface._get_day_slice(0)\n",
        "\n",
        "    config = start_config.copy()\n",
        "    config.update({\n",
        "        \"batch_size\": tune.choice([16, 32, 64]),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"momentum\": tune.uniform(0.1, 0.9),\n",
        "    })\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"mean_accuracy\",\n",
        "        mode=\"max\",\n",
        "        max_t=num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        parameter_columns=[\"lr\", \"momentum\", \"batch_size\"],\n",
        "        metric_columns=[\"mean_accuracy\", \"training_iteration\"])\n",
        "\n",
        "    analysis = tune.run(\n",
        "        partial(\n",
        "            train_mnist,\n",
        "            start_model=start_model,\n",
        "            data_fn=data_interface.get_incremental_data,\n",
        "            num_epochs=num_epochs,\n",
        "            use_gpus=True if gpus_per_trial > 0 else False,\n",
        "            day=day),\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 1,\n",
        "            \"gpu\": gpus_per_trial\n",
        "        },\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter,\n",
        "        verbose=0,\n",
        "        name=\"tune_serve_mnist_fromsexisting\")\n",
        "\n",
        "    best_trial = analysis.get_best_trial(\"mean_accuracy\", \"max\", \"last\")\n",
        "    best_accuracy = best_trial.metric_analysis[\"mean_accuracy\"][\"last\"]\n",
        "    best_trial_config = best_trial.config\n",
        "    best_checkpoint = best_trial.checkpoint.value\n",
        "\n",
        "    return best_accuracy, best_trial_config, best_checkpoint, num_examples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPjq9qGuvN_A"
      },
      "source": [
        "You can change the num of day, and train from existing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里我们需要一个 def get_current_model(model_dir)来使用旧的配置和模型，不用重新开始训练了。接着就可以进行持续学习了并将它更新到你的Ray Serve上了。"
      ],
      "metadata": {
        "id": "i7n8spxPZXoi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHtF706jyGet",
        "outputId": "108fbb88-3826-459e-f5c0-00640c8713b3"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    day = 4\n",
        "    ray.shutdown()\n",
        "    ray.init(namespace=\"tune-serve-integration\")\n",
        "\n",
        "    model_dir = os.path.expanduser(\"~/mnist_tune_serve\")\n",
        "\n",
        "    gpus_per_trial = 0.25\n",
        "    serve_gpu = True if gpus_per_trial > 0 else False\n",
        "    num_samples = 1\n",
        "    num_epochs = 1\n",
        "\n",
        "\n",
        "    def get_current_model(model_dir):\n",
        "        checkpoint_path = os.path.join(model_dir, \"checkpoint\")\n",
        "        meta_path = os.path.join(model_dir, \"meta.json\")\n",
        "\n",
        "        if not os.path.exists(checkpoint_path) or \\\n",
        "                not os.path.exists(meta_path):\n",
        "            return None, None, None\n",
        "\n",
        "        with open(meta_path, \"rt\") as fp:\n",
        "            meta = json.load(fp)\n",
        "\n",
        "        return checkpoint_path, meta[\"config\"], meta[\"metrics\"]\n",
        "\n",
        "    # train from existing\n",
        "    old_checkpoint, old_config, old_acc = get_current_model(model_dir)\n",
        "    if not old_checkpoint or not old_config or not old_acc:\n",
        "            print(\"No existing model found. Train one with --from_scratch \"\n",
        "                  \"first.\")\n",
        "            sys.exit(1)\n",
        "    acc, config, best_checkpoint, num_examples = tune_from_existing(\n",
        "            old_checkpoint,\n",
        "            old_config,\n",
        "            num_samples,\n",
        "            num_epochs,\n",
        "            gpus_per_trial,\n",
        "            day=day)\n",
        "    print(\"Trained day {} from existing on {} samples. \"\n",
        "              \"Best accuracy: {:.4f}. Best config: {}\".format(\n",
        "            day, num_examples, acc, config))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-12 13:03:49,430\tWARNING experiment.py:256 -- No name detected on trainable. Using DEFAULT.\n",
            "2022-01-12 13:03:49,437\tINFO registry.py:70 -- Detected unknown callable for trainable. Converting to class.\n",
            "2022-01-12 13:03:51,716\tWARNING worker.py:1245 -- Warning: The actor ImplicitFunc is very large (45 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
            "2022-01-12 13:03:52,343\tWARNING util.py:166 -- The `start_trial` operation took 1.253 s, which may be a performance bottleneck.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained day 4 from existing on 22200 samples. Best accuracy: 0.9412. Best config: {'batch_size': 16, 'layer_size': 64, 'lr': 0.01753766087795641, 'momentum': 0.1121616438238826}\n"
          ]
        }
      ]
    }
  ]
}